{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f05d60ea2d15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Hyper Parameters\n",
    "\n",
    "class Hp():\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.nb_steps = 1000 # no of training loops we are going to have in the end or no of times\n",
    "        # we are going to update our model.\n",
    "        self.episode_length = 1000 # maximum length of an episode, i.e maximum time AI will walk on\n",
    "        # the field.\n",
    "        self.learning_rate = 0.02 # learning rate to control how fast AI is learning\n",
    "        self.nb_directions = 16 # no of pertubations to be applied on each of these weights. More\n",
    "        # directions to explore, more the reward, but will take more training time.\n",
    "        self.nb_best_directions = 16 # choosing the best no of directions\n",
    "        assert self.nb_best_directions <= self.nb_directions # We ensure that no of best directions\n",
    "        # is less than total no of directions\n",
    "        self.noise = 0.03 # sigma of the Gaussian Distribution which will be used to sample the \n",
    "        # pertubations\n",
    "        self.seed = 1 # seed to fix the current configuration of environment\n",
    "        self.env_name = 'HalfCheetahBulletEnv-v0' # setting the environment name\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here states are some vectors describing perfecly at each time what is happening at time t. So each\n",
    "So each state is represented by a vector, which will contain the coordinates of different points of\n",
    "virtual robots, and each vector describes perfectly what's happening in the environment, so that we can draw the picture by only looking at the environment. And that will be the input for the perceprton.\n",
    "<br>\n",
    "We need to normalise the states to increase the sensitivity of the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the states\n",
    "\n",
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, nb_inputs):\n",
    "        # inputs to the perceptron\n",
    "        self.n = np.zeros(nb_inputs) # counter which tells how many states we have encountered\n",
    "        self.mean = np.zeros(nb_inputs) # mean of values of each input vector\n",
    "        self.mean_diff = np.zeros(nb_inputs) \n",
    "        self.var = np.zeros(nb_inputs)\n",
    "    \n",
    "    # Update mean and variance on seeing a new state\n",
    "    \n",
    "    def observe(self, x):\n",
    "        self.n += 1. # since  we update on seeing a new state, n increases everytime by 1\n",
    "        # Mean Computation. It will be not be simple mean calculation. It will be an online mean. \n",
    "        # Each time you have a previous mean, and when a new value comes, we need to update the \n",
    "        # mean.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2) # To make sure variance is never 0\n",
    "    \n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our AI is a policy, which is a function that taking inputs, the states of environments, and returning some actions to play, in order to work. This algo basically focusses on exploration of space of pulses. We will be exploring a lot of policies and converging to a one, that will return the best actions to work. \n",
    "<br>\n",
    "We will build the policy and give it tools to update, during the training by applying some pertubations and updating weights of the policy, in the direction of rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def evaluate(self, input, delta = None, direction = None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + hp.noise*delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - hp.noise*delta).dot(input)\n",
    "    \n",
    "    def sample_deltas(self):\n",
    "        # We need to return pertubations, because there are 16 directions.\n",
    "        # These are matrices following normal distribution having mean zero and variance 1\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.nb_directions)]\n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r) * step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the policy on one specific direction and over one episode\n",
    "\n",
    "def explore(env, normalizer, policy, direction = None, delta = None):\n",
    "    state = env.reset() # Rsetting the environment\n",
    "    done = False # To mark start and end of an episode.\n",
    "    num_plays = 0.\n",
    "    sum_rewards = 0\n",
    "    while not done and num_plays < hp.episode_length:\n",
    "        # Normalization\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalize(state)\n",
    "        # Feeding data to the perceptron\n",
    "        action = policy.evaluate(state, delta, direction)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # Not to be biased by super high reward or a super low reward.\n",
    "        reward = max(min(reward, 1), -1) # Classic trick in Reinforcemet Learning.\n",
    "        # We enforce super high and super low rewards to be +1 and -1 respectively.\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    return sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, policy, normalizer, hp):\n",
    "    \n",
    "    for step in range(hp.nb_steps):\n",
    "        \n",
    "        # Initializing the perturbations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas()\n",
    "        positive_rewards = [0] * hp.nb_directions\n",
    "        negative_rewards = [0] * hp.nb_directions\n",
    "        \n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "            # delta[k] represents the pertubation applied in the kth direction\n",
    "        # Getting the negative rewards in the negative/opposite directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            negative_rewards[k] = explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "        \n",
    "        # Gathering all the positive/negative rewards to compute the standard deviation of these rewards\n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "        \n",
    "        # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "        \n",
    "        # For all 16 directions, we take maximum of positive and negative rewards.\n",
    "        scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:hp.nb_best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "        \n",
    "        # Updating our policy\n",
    "        policy.update(rollouts, sigma_r)\n",
    "        \n",
    "        # Printing the final reward of the policy after the update\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print('Step:', step, 'Reward:', reward_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the main code\n",
    "\n",
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Hp()\n",
    "np.random.seed(hp.seed)\n",
    "env = gym.make(hp.env_name)\n",
    "env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "policy = Policy(nb_inputs, nb_outputs)\n",
    "normalizer = Normalizer(nb_inputs)\n",
    "train(env, policy, normalizer, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
